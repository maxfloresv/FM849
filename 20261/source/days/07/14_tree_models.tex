%Modelos de aprendizaje supervisado basados en árboles: Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost.

%Más modelos de aprendizaje supervisado: k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), Naïve Bayes. Intuición detrás de las variantes de SVM y Naïve Bayes.

\documentclass[11pt,t,aspectratio=169]{beamer}
\usepackage{borelian}


\begin{document}
\classtitle{14}{Modelos Clasicos de Machine Learning}



%%%%%%%
\begin{frame}{Contenidos de esta clase}
    Algunos de los modelos mas utilizados en Machine Learning:
    
    \begin{itemize}%[<+->]
        \item Decision Tree.
        \item Metodos basados en Ensemble.
        \item XGBoost.
        %\item LightGBM.
        %\item CatBoost. 
    \end{itemize}
    
\end{frame}


%%%%%%%
\begin{frame}{Recap: Que modelos hemos estudiado?}    

Hasta ahora hemos estudiado \textbf{K-Nearest Neighbour, Naive Bayes y Support Vector Machine}.

\vspace{5mm}

En esta clase estudiaremos \textbf{modelos basados en arboles}. Estos modelos son bastante flexibles y funcionan en datasets no lineales. Adicionalmente, los arboles son \textbf{interpretables} y podemos ver cuales \textbf{features} son mas relevantes para el modelo.
\end{frame}


\begin{frame}{Modelos basados en arboles}    

Este tipo de modelos dividen los datos múltiples veces según valores limites en las features. Estas divisiones crean subconjuntos de los datos. Los últimos subconjuntos generados son llamados nodos terminales.

%\begin{itemize}
%    \item Permiten establecer interacciones entre las features.
%    \item Tienen representaciones que permiten visualizar las decisiones que el modelo toma al clasificar.
%\end{itemize}

\vspace{5mm}

\begin{tikzpicture}[
    every node/.style={draw, rectangle, rounded corners, align=center},
    level distance = 2cm,
    level 1/.style={sibling distance=6cm},
    level 2/.style={sibling distance=6cm},
    edge from parent/.style={draw, -latex}
]

% Pregunta de clasificación
\node[draw=none, font=\bfseries] (q) {¿Jugar al aire libre?};
\\
% Árbol
\node[below=5cm of q,yshift= -0.5cm] {¿Clima?}
    child { node {Decisión\\\textbf{No}}
        edge from parent node[left] {Lluvioso}
    }
    child { node {¿Temperatura?}
        child { node {Decisión\\\textbf{No}}
            edge from parent node[left] {Alta}
        }
        child { node {Decisión\\\textbf{Sí}}
            edge from parent node[right] {Baja}
        }
        edge from parent node[right] {Soleado}
    };
\end{tikzpicture}
\end{frame}



\begin{frame}{Metricas de Pureza}
\textbf{Idea:} Mide que tan puro o impuro es el dataset. 

\begin{itemize}
\item \textbf{Gini Impurity}: Dado un conjunto de puntos $S=\{(x_1,y_1),...,(x_n,y_n) \}$. La probabilidad de obtener una label es $p_{k}=\frac{|S_k|}{S}$.

\[
S_k = \{(x,y) \in S ~|~ y=k\}
\]

Luego, podemos definir la metrica de impureza Gini: $G(S)=\sum_{k=1} p_k \cdot(1-p_k)$

%\item \textbf{Entropía}:

\end{itemize}


    
\end{frame}



\begin{frame}{Decision Tree: Como Realiza Predicciones?}
\begin{columns}
    \column{0.7\textwidth}
\begin{itemize}[<+->]
    \item La predicción comienza en la \textbf{raíz} del árbol (profundidad 0).
    \item Cada nodo formula una pregunta sobre una característica (ej.: longitud del pétalo).
    \item Si la condición se cumple, se avanza al \textbf{hijo izquierdo}; si no, al \textbf{hijo derecho}.
    \item El proceso continúa hasta llegar a un \textbf{nodo hoja}.
    \item El nodo hoja asigna la clase más frecuente entre los ejemplos que llegan a él.
\end{itemize}

    \column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{days/07/images_14/decision_tree.png}
    %\caption{Caption}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9
    \linewidth]{days/07/images_14/decision_tree2.png}
    %\caption{Caption}
    \label{fig:placeholder}
\end{figure}

\end{columns}
\end{frame}


\begin{frame}{Decision Tree: Como Se Entrena/Construye?}
Uno de los algoritmos mas comunes es CART (Classification And Regression Tree).
\begin{columns}
    \column{0.8\textwidth}
\begin{itemize}[<+->]
    \item En cada nodo, CART busca una característica $k$ y un umbral $t_k$ que divide los datos en dos subconjuntos.
    \item El criterio de selección de $(k,t_k)$ minimiza una \textbf{función de costo} basada en la impureza:
    \[
    J(k,t_k) = \frac{m_{\text{left}}}{m} G_{\text{left}} + \frac{m_{\text{right}}}{m} G_{\text{right}}
    \]
    \item El proceso se aplica \textbf{recursivamente} a cada subconjunto.
\end{itemize}

    \column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{}
    %\caption{Caption}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9
    \linewidth]{}
    %\caption{Caption}
    \label{fig:placeholder}
\end{figure}

\end{columns}

\vspace{5mm}

Cuando se detiene el algoritmo? Según hiperparametros como la \textbf{prof. máxima, número de datos en las hojas, etc.}

\end{frame}






\begin{frame}{Ensemble Learning: Bagging and Pasting}
\begin{itemize}
    \item \textbf{Idea:} Utilizar muchos modelos predictores y agregar sus respuestas.

    \item Es esperable que esta respuesta "promedio" sea mejor que la de un modelo único.

\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{days/07/images_14/ensemble_learning.png}
    \caption{Bagging (con reemplazo) y Pasting (sin reemplazo). Fuente:}
    \label{fig:placeholder}
\end{figure}

\textbf{Ejemplo}: Es posible entrenar un grupo de Decision Trees, cada uno en un subconjunto de los datos. Luego cuando queremos generar una predicción en un dato nuevo, generamos predicciones con todos y promediamos sus respuestas.

\end{frame}


\begin{frame}{Random Forest}

\begin{itemize}
    \item El modelo de Random Forest es un ejemplo de emsemble learning. En Random Forest se entrenan multiples Decision Trees tipicamente usando Bagging.

\end{itemize}

from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)


El modelo Random Forest añade extra aleatoriedad en el proceso de crecimiento (CART) de los arboles. En vez de buscar la mejor feature para dividir los datos, selecciona la mejor dentro un subset del total de features.

\end{frame}


\begin{frame}{Boosting y AdaBoost}

\textbf{Idea:} Entrenar secuencialmente modelos predictores, en el que el modelo actual entrenado corrige los errores del predecesor.

\textbf{Adaboost}

Una manera de generar nuevos y mejores modelos es entrenar enfocandose en datos donde el modelo predecesor comete errores.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{days/07/images_14/adaboost.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}



\end{frame}


\begin{frame}{Gradient Boosting}
\begin{itemize}
    \item Similar a AdaBoost, Gradient Boosting entrena modelos predictores secuencialmente. Cada modelo corrige los errores de su predecesor.

    \item La diferencia esta en que en vez de aumentar el peso de los ejemplos erroreneos, Gradient Boosting ajusta los parametros del nuevo modelo considerando los errores residuales del modelo anterior.

\end{itemize}
\end{frame}




\begin{frame}{Resumen de Ventajas y Desventajas de cada algoritmo}
\small

\begin{table}
\centering
\begin{tabular}{c|p{6cm}|p{6cm}}
\hline
\textbf{Modelo} & \textbf{Ventaja} & \textbf{Desventaja} \\
\hline
kNN 
& No requiere entrenamiento explícito. 
& Sensible a la dimensionalidad. \\
NB 
& Eficiente incluso en alta dimensión. 
& Independencia condicional poco realista. \\
SVM 
& Data efficient ($\sim$ 1000). %en espacios de alta dimensión 
& Costoso y sensible a hiperparámetros. \\
\hline
\end{tabular}
\end{table}



\end{frame}





\begin{frame}{Referencias:}
    \begin{itemize}
        \item https://medium.com/data-science/the-math-behind-the-curse-of-dimensionality-cf8780307d74

        \item Geron. 2022 .Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools. 
    \end{itemize}
\end{frame}


\end{document}

%%%%%%%

        
        

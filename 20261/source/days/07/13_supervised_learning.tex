%Más modelos de aprendizaje supervisado: k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), Naïve Bayes. Intuición detrás de las variantes de SVM y Naïve Bayes.

\documentclass[11pt,t,aspectratio=169]{beamer}
\usepackage{borelian}


\begin{document}
\classtitle{12}{Modelos Clasicos de Aprendizaje Supervisado}



%%%%%%%
\begin{frame}{Contenidos de esta clase}
    Algunos de los modelos mas utilizados en Machine Learning:
    
    \begin{itemize}%[<+->]
        \item Recuerdo sobre Aprendizaje Supervisado.
        \item Support Vector Machine (SVM).
        \item k-Nearest Neighbors (k-NN)
        \item Naïve Bayes.

    \end{itemize}
    
\end{frame}


%%%%%%%
\begin{frame}{Recuerdo de Aprendizaje Supervisado}    
%El problema de aprendizaje supervisado es el mas comun en IA. Este consiste en a partir de ejemplos etiquetados optimizar los parametros de un modelo $f_{\theta}$ de tal forma que este modelo represente correctamente la distribucion de los datos.\newline

%Dado un conjunto de ejemplos $\{(x_i, y_i)\}_{i=1}^N$, el objetivo es optimizar los parámetros de un modelo $f_{\theta}$ de modo que este capture la relación entre los datos de entrada y sus etiquetas.

\vspace{0.3cm}

%Cuando decimos que el modelo \emph{representa la distribución de los datos}, nos referimos a que, dado un nuevo input $x \in \mathcal{X}$, el modelo sea capaz de generar una predicción $\hat{y} = f_{\theta}(x)$ que sea consistente con los valores observados en los datos de entrenamiento.

\textbf{Idea General}: Tenemos pares features y etiquetas $\{ \overrightarrow{x}_i,y_i\}$. Queremos encontrar un modelo $h$, al cual le entregaremos features de un ejemplo $\overrightarrow{x}_i$ y retornara la etiqueta $y_i$.

Esto es lo mismo que decir \[ h(\overrightarrow{x}_i)=y_i \]

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{days/07/images/ml_model.pdf}
    \caption{Esquema de un modelo ML en aprendizaje supervisado.}
    \label{fig:placeholder}
\end{figure}

%\vspace{-5mm}

%\textbf{Importante}: Dependiendo del dataset y la distribución de los datos cierto tipo de modelo sera mas útil que otro.

\end{frame}



%%%%%%%
\begin{frame}{Recuerdo de Aprendizaje Supervisado}    
Estudiaremos diferentes tipos de modelos utilizados para resolver el problema de clasificación. Empezaremos dando una idea general, luego un ejemplo de como utilizar el modelo para clasificar y finalmente hablaremos de detalles que existen en cada modelo.

\begin{itemize}%[<+->]
    \item k-Nearest Neighbors (k-NN)
    \item Naïve Bayes.
    \item Support Vector Machine (SVM).
\end{itemize}
    

\end{frame}


\begin{frame}{Naive Bayes}

\textbf{Idea:} Dado un conjunto de features vamos a generar una probabilidad de ocurrencia para cada clase $y_i$.\newline

Dado un input $x$ de $n$ features $x = [x_1,x_2,...,x_n]$ queremos obtener $P(y_i| x_1,x_2,...,x_n)$. Para hacer esto deberiamos buscar los casos dado $x$ ocurre $y_i$, lo cual es intratable computacionalmente.\newline

La idea sera entonces convertir la expresión $P(y_i|x)$ en algo mas simple y calculable. Usamos dos cosas: Teorema de Bayes e Independencia Condicional.

\vspace{5mm}
\begin{itemize}
    \item Teorema de Bayes:
    $P(A|B) = \frac{ P(B|A) \cdot P(A) }{P(B)}$
    %Sabemos de probabilidades que $P(A|B) = \frac{P( A\cap B)  }{P(B)}$ y $P(B|A) = \frac{P( A\cap B)  }{P(A)}$. Entonces:
%\[  \Rightarrow P( A\cap B) = P(B|A) \cdot P(A)  \]
%\[  \Rightarrow P( A | B) = \frac{P( A\cap B)  }{P(B)} = \frac{P(B|A) \cdot P(A)  }{P(B)}  \]
    
    \item Independencia Condicional: Dada una clase $y_i$, los features $x_1,...,x_n$ ocurren de manera independiente.
    
\end{itemize}


\end{frame}


\begin{frame}{Naive Bayes}

Entonces usando el Teorema de Bayes tenemos que:
\[
P(y_i| x_1,x_2,...,x_n) = \frac{P(x_1,x_2,...,x_n|y_i) P(y_i) }{P(x_1,x_2,...,x_n)}
\]
Usando la independencia condicional entre los features llegamos a:
\[
P(y_i| x_1,x_2,...,x_n) = \frac{ P(x_1|y_i) \cdot P(x_2|y_i) \cdot \cdot  \cdot P(x_n|y_i)  \cdot P(y_i) }{P(x_1,x_2,...,x_n)}
\]
Podemos ver que el denominador es una constante para todas las clases del problema, entonces:
\[
\hat{y} = \arg \max_{y_i~\in Y} P(x_1|y_i) \cdot P(x_2|y_i) \cdot \cdot  \cdot P(x_n|y_i)  \cdot P(y_i) 
\]

\end{frame}


\begin{frame}{Naive Bayes: Ejemplo}
\small

Revisemos un ejemplo para entender la idea de Naive Bayes. 
\url{https://docs.google.com/presentation/d/1oFK21SaZLpy28ArDHc7CaM0GWLVksyOxqw27pcC7DcM/edit?usp=sharing}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% KNN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{k-Nearest Neighbors (kNN)}
\textbf{Idea:} Clasificamos un dato según que tan similar es todo nuestro dataset.
\begin{itemize}
    %\item Usa los $k$ puntos más cercanos (nearest neighbors) para realizar la clasificación.
    \item Dado un ejemplo desconocido a clasificar, busca los $k$ ejemplos conocidos más cercanos. Por defecto, cada clase suma 1 voto si un ejemplo suyo está entre los $k$ más cercanos.
    \item El nuevo ejemplo es clasificado con la clase de mayor votación.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{days/07/images/knn1.png}
    %\caption{Caption}
    \label{fig:placeholder}
\end{figure}
\end{frame}


\begin{frame}{k-Nearest Neighbors (kNN)}

Tenemos que \textbf{buscar} los ejemplos más cercanos. \textbf{Como hacemos esto ?} Necesitamos una métrica de distancia para buscarlos ejemplos mas cercanos.% Exiten multiples tipos de distancias:

\vspace{5mm}
\begin{itemize}
    \item Distancia euclidiana.
    \[
    d_{eucli} (x,z) = \sqrt{\sum^{d}_{i=1} (x_i - z_i)^2 }
    \]
\end{itemize}

\vspace{5mm}
\textbf{Problema:} Es importante escalar los datos para esta métrica, pues ciertas dimensiones en los datos pueden tener mayores magnitudes.
    
\end{frame}



\begin{frame}{Ejemplo de KNN}

Vamos a ver un ejemplo sobre prediccion de la especie de pinguinos. \url{https://docs.google.com/presentation/d/1pbGDrzTU_X1yop1pkZ3rE3dJkxAQL5mLzGHv42PNERo/edit?usp=sharing}

Supongamos que tenemos un dataset con dos especies de pingüinos.
Cada pingüino se describe mediante dos características numéricas.

\medskip

\begin{table}
\centering
\begin{tabular}{l c c c}
\hline
\textbf{Especie} & \textbf{Peso (kg)} & \textbf{Altura (cm)} & \textbf{Clase} \\
\hline
Emperador & 35 & 115 & Emperador \\
Emperador & 32 & 110 & Emperador \\
\vdots & \vdots & \vdots & \vdots \\
Humboldt & 5 & 65 & Humboldt \\
Humboldt & 4.8 & 62 & Humboldt \\
\hline
\end{tabular}
\caption{Ejemplo de dataset de pingüinos}
\end{table}



\end{frame}


\begin{frame}{KNN: problema de escalamiento}

En KNN, la clasificación se basa en \textbf{distancias} entre ejemplos.

\medskip

Ejemplo: cada pingüino se describe por
\begin{itemize}
    \item Peso (kg)
    \item Altura (cm)
\end{itemize}

\medskip

Punto a clasificar:
\[
x = (30,\;100)
\]

Dos ejemplos del dataset:
\[
z_1 = (32,\;110) \quad \text{(Emperador)}
\]
\[
z_2 = (5,\;65) \quad \text{(Humboldt)}
\]

\medskip

\textbf{Problema:}  
La altura (cm) tiene valores mucho mayores que el peso (kg), por lo que
\textbf{domina la distancia euclidiana} y el peso tiene poco impacto.

\end{frame}


\begin{frame}{KNN: ¿por qué escalar?}

Para que ambas características influyan de manera comparable,
escalamos los datos (por ejemplo, con normalización min--max):

\[
x_i^{\text{norm}} =
\frac{x_i - \min(x_i)}{\max(x_i) - \min(x_i)}
\]

\medskip

Tras escalar:
\begin{itemize}
    \item Peso y altura quedan en el mismo rango
    \item Ninguna variable domina a la otra
    \item KNN decide usando información real, no las unidades de medida
\end{itemize}

\end{frame}






%\begin{frame}{KNN: Problema de la dimensionalidad}
%En adición al problema de escala en KNN, existe problemas cuando usamos dimensiones.
%En altas dimensiones, las funciones de distancia pierden
%la capacidad de distinguir elementos que están muy cerca o muy lejos. Las distancias se empiezan
%a concentrar alrededor de un valor.
%
%\begin{figure}
%    \centering
%    \includegraphics[width=0.4\linewidth]{days/07/images/distances_max_min.png}
%    \caption{Variación entre distancia minima y maxima para datos aleatorios con diferente numero de dimensiones. Fuente: \url{https://medium.com/data-science/the-math-behind-the-curse-of-dimensionality-cf8780307d74}}
%    \label{fig:placeholder}
%\end{figure}   
%\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SVM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{frame}{Support Vector Machine}

\textbf{Idea:} Generar un espacio de separación (hiperplano) perpendicular a los datos que separe las clases.

\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{days/07/images/hiperplanos_svm.png}
    \caption{Ejemplo de hiperplanos. Fuente: \url{https://en.wikipedia.org/wiki/Support_vector_machine}.}
    \label{fig:placeholder}
\end{figure}    
\end{frame}


\begin{frame}{Support Vector Machine}

\begin{columns}
    
    \column{0.5\textwidth} 
    \begin{itemize}
    \item Corresponde a un método que busca separar las clases con un hiperplano (en 2D, una recta).
    \item El hiperplano elegido es el que maximiza el margen que se genera con los vectores de soporte.
    \item Cada vez que evaluamos un nuevo punto en la ecuación del hiperplano $w\cdot x +b = 0 $, clasificamos según el signo de $w\cdot x +b$.
    \end{itemize}

    \column{0.5\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{days/07/images/svm.png}
    \caption{Ilustración de SVM.}
    \label{fig:placeholder}
\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Problema de Outliers en Support Vector Machine}

Los \textbf{outliers} no permiten generar una separación perfecta cuando usamos SVM.

\begin{columns}
    
    \column{0.5\textwidth} 
    \begin{itemize}
    \item En esos casos conviene generar un modelo mas flexible, es decir, le permitimos equivocarse en algunos ejemplos.
    \item La idea sera hacer la "calle" tan larga como sea posible con la menor cantidad de ejemplos errados. Esto se controla con un hiperparametro $C$ del modelo.
    \item Esta técnica es comúnmente llamada \emph{soft Margin}.
    \end{itemize}

    \column{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{days/07/images/ouliers_svm.png}
    \caption{Ejemplos de datos con outliers.}
    \label{fig:placeholder}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{days/07/images/svn_outliers2.png}
    \caption{Modelo con \emph{soft Margin}.}
    \label{fig:placeholder}
\end{figure}
\end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%% EJEMPLO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Ejemplo 1: Support Vector Machine}
Supongamos que tenemos una tabla de datos con información del peso de dos especies de pingüinos.
Cada fila representa un pingüino y su especie corresponde a la clase a predecir.
\vspace{0.3cm}
\begin{table}
\centering
\begin{tabular}{c c}
\hline
\textbf{Peso (kg)} & \textbf{Especie} \\
\hline
\rowcolor{green!20} 32 & Emperador \\
\rowcolor{green!20} 35 & Emperador \\
\rowcolor{green!20} 38 & Emperador \\
\rowcolor{red!20}  12 & Humbolt \\
\rowcolor{red!20}  14 & Humbolt \\
\rowcolor{red!20}  16 & Humbolt \\
\hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Ejemplo 1: Support Vector Machine}

Supongamos que tenemos una tabla de datos con información del peso de dos especies de pingüinos.
Ahora agregamos un nuevo pingüino cuya especie es desconocida.

\vspace{0.3cm}
\begin{table}
\centering
\begin{tabular}{c c}
\hline
\textbf{Peso (kg)} & \textbf{Especie} \\
\hline
\rowcolor{green!20} 32 & Emperador \\
\rowcolor{green!20} 35 & Emperador \\
\rowcolor{green!20} 38 & Emperador \\
\rowcolor{red!20}  12 & Humbolt \\
\rowcolor{red!20}  14 & Humbolt \\
\rowcolor{red!20}  16 & Humbolt \\
\rowcolor{black!15} \textcolor{black}{25} & \textcolor{black}{?} \\
\hline
\end{tabular}
\end{table}

Qué especie es el pinguino en la ultima fila ?

\vspace{0.3cm}
\textbf{Animated version:}  
\href{https://docs.google.com/presentation/d/1A4FXPfxkzYsJ-V-HleIUiZJ4CGTqPtCJP0kQICOpGvk/edit?usp=sharingL}{Google Slides with animations}

\end{frame}

\begin{frame}{Support Vector Machine: Kernel}

Existen casos en los cuales los datos no son linearmente separables (por ejemplo Figura \ref{fig:kernel_svm}). En estos casos, SVM utiliza una herramienta matematica llamada \textbf{kernel}. \newline

El kernel transforma el espacio original de input features en un nuevo espacio vectorial, de tal forma de poder separar los datos. 

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{days/07/images/kernel_svm.png}
    %\caption{}
    \label{fig:kernel_svm}
\end{figure}

\end{frame}


\begin{frame}{Ejemplo 2: Support Vector Machine}

Supongamos que tenemos una tabla de datos con información del peso de dos especies de pingüinos.
Ahora agregamos un nuevo pingüino cuya especie es desconocida. 

\vspace{0.3cm}
\begin{table}
\centering
\begin{tabular}{c c}
\hline
\textbf{Peso (kg)} & \textbf{Especie} \\
\hline
\rowcolor{green!20} 32 & Emperador \\
\rowcolor{green!20} 35 & Emperador \\
\rowcolor{green!20} 38 & Emperador \\
\rowcolor{red!20}  12 & Humbolt \\
\rowcolor{red!20}  14 & Humbolt \\
\rowcolor{red!20}  16 & Humbolt \\
\rowcolor{black!15} \textcolor{black}{25} & \textcolor{black}{?} \\
\hline
\end{tabular}
\end{table}

Qué especie es el pinguino en la ultima fila ?

\vspace{0.3cm}
\textbf{Animated version:}  
\href{https://docs.google.com/presentation/d/1A4FXPfxkzYsJ-V-HleIUiZJ4CGTqPtCJP0kQICOpGvk/edit?usp=sharingL}{Google Slides with animations}

\end{frame}



\begin{frame}{Resumen de Ventajas y Desventajas de cada algoritmo}
\small

\begin{table}
\centering
\begin{tabular}{c|p{6cm}|p{6cm}}
\hline
\textbf{Modelo} & \textbf{Ventaja} & \textbf{Desventaja} \\
\hline
kNN 
& No requiere entrenamiento explícito. 
& Sensible a la dimensionalidad. \\
NB 
& Eficiente incluso en alta dimensión. 
& Independencia condicional poco realista. \\
SVM 
& Data efficient ($\sim$ 1000). %en espacios de alta dimensión 
& Costoso y sensible a hiperparámetros. \\
\hline
\end{tabular}
\end{table}



\end{frame}





\begin{frame}{Referencias:}
    \begin{itemize}
        \item https://medium.com/data-science/the-math-behind-the-curse-of-dimensionality-cf8780307d74
        
    \end{itemize}
\end{frame}


\end{document}